新浪科技讯6月14日下午消息，第六届“北京智源大会”在中关村展示中心开幕。智源研究院院长王仲远介绍了智源研究院在语言、多模态、具身、生物计算大模型的前沿探索和研究进展，<cand>同时</cand>宣布推出智源大模型全家桶及全栈开源技术基座新版图。
据介绍，针对大模型训练算力消耗高的问题，智源研究院和中国电信人工智能研究院（TeleAI）基于模型生长和损失预测等关键技术，联合研发并推出全球首个低碳单体稠密万亿语言模型Tele-FLM-1T。该模型与百亿级的52B版本，千亿级的102B版本共同构成Tele-FLM系列模型。
针对大模型幻觉等问题，智源研究院自主研发了通用语义向量模型BGE（BAAIGeneralEmbedding）系列，基于检索增强RAG技术，实现数据之间精准的语义匹配，支持大模型调用外部知识的调用。自2023年8月起，BGE模型系列先后进行了三次迭代，分别在中英文检索、多语言检索、精细化检索三个任务中取得了业内最佳表现，综合能力显著优于OpenAI、Google、Microsoft、Cohere等机构的同类模型。
<cand>此外</cand>，<cand>为了</cand>实现多模态、统一、端到端的下一代大模型，智源研究院推出了Emu3原生多模态世界模型。Emu3采用智源自研的多模态自回归技术路径，在图像、视频、文字上联合训练，<cand>使</cand>模型具备原生多模态能力，实现了图像、视频、文字的统一输入和输出。
王仲远表示，现阶段语言大模型的发展已经具备了通用人工智能非常核心的理解和推理能力，<cand>并且</cand>形成了一条以语言大模型为核心对齐和映射其他模态的技术路线，<cand>从而</cand>让模型具备了初步的多模态理解和生成能力。<cand>但</cand>这并<a>不是</a>让人工智能感知、理解物理世界的终极技术路线，<a>而是</a>应该采取统一模型的范式，实现多模态的输入和输出，<cand>让</cand>模型具备原生的多模态扩展能力，向世界模型演进。
